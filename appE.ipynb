{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54acd9fe-a259-431f-9e2a-65157860bd4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:02.385264Z",
     "iopub.status.busy": "2025-03-31T00:27:02.384853Z",
     "iopub.status.idle": "2025-03-31T00:27:03.293911Z",
     "shell.execute_reply": "2025-03-31T00:27:03.293554Z",
     "shell.execute_reply.started": "2025-03-31T00:27:02.385237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/tmp/sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from chap6 import download_and_unzip_spam_data, create_balanced_dataset, random_split\n",
    "\n",
    "base_path = '/work/tmp'\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = f\"{base_path}/sms_spam_collection.zip\"\n",
    "extracted_path = f\"{base_path}/sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "\n",
    "df = pd.read_csv(\n",
    "    data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"]\n",
    ")\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(f\"{base_path}/train.csv\", index=None)\n",
    "validation_df.to_csv(f\"{base_path}/validation.csv\", index=None)\n",
    "test_df.to_csv(f\"{base_path}/test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc9f800-12d0-4a45-971d-b5236bd47583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:03.294365Z",
     "iopub.status.busy": "2025-03-31T00:27:03.294259Z",
     "iopub.status.idle": "2025-03-31T00:27:03.416754Z",
     "shell.execute_reply": "2025-03-31T00:27:03.416187Z",
     "shell.execute_reply.started": "2025-03-31T00:27:03.294357Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tiktoken\n",
    "from chap6 import SpamDataset\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(f\"{base_path}/train.csv\", max_length=None, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "val_dataset = SpamDataset(f\"{base_path}/validation.csv\", \n",
    "    max_length=train_dataset.max_length, tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    f\"{base_path}/test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39161e6-e009-45af-a1ec-32894277d91f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:03.417095Z",
     "iopub.status.busy": "2025-03-31T00:27:03.417006Z",
     "iopub.status.idle": "2025-03-31T00:27:03.419929Z",
     "shell.execute_reply": "2025-03-31T00:27:03.419635Z",
     "shell.execute_reply.started": "2025-03-31T00:27:03.417085Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb2c3e1-cf0d-4b49-bd34-0fff986ad16b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:03.420273Z",
     "iopub.status.busy": "2025-03-31T00:27:03.420184Z",
     "iopub.status.idle": "2025-03-31T00:27:03.461463Z",
     "shell.execute_reply": "2025-03-31T00:27:03.461074Z",
     "shell.execute_reply.started": "2025-03-31T00:27:03.420265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c4abbe4-7249-4bfd-af13-8bf662e3fa8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:03.461797Z",
     "iopub.status.busy": "2025-03-31T00:27:03.461717Z",
     "iopub.status.idle": "2025-03-31T00:27:03.463930Z",
     "shell.execute_reply": "2025-03-31T00:27:03.463671Z",
     "shell.execute_reply.started": "2025-03-31T00:27:03.461790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c136547f-8548-4190-9bbe-a203466de15d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:03.464764Z",
     "iopub.status.busy": "2025-03-31T00:27:03.464636Z",
     "iopub.status.idle": "2025-03-31T00:27:03.604684Z",
     "shell.execute_reply": "2025-03-31T00:27:03.604342Z",
     "shell.execute_reply.started": "2025-03-31T00:27:03.464756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if torch.backends.mps.is_available():   #1\n",
    "#     device = torch.device(\"mps\")\"      \n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbf2c497-4502-4f9a-b77a-9c497ac313de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:03.604976Z",
     "iopub.status.busy": "2025-03-31T00:27:03.604892Z",
     "iopub.status.idle": "2025-03-31T00:27:11.406000Z",
     "shell.execute_reply": "2025-03-31T00:27:11.405721Z",
     "shell.execute_reply.started": "2025-03-31T00:27:03.604968Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 00:27:03.763812: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-31 00:27:03.771596: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743380823.780730    2525 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743380823.783496    2525 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743380823.790789    2525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743380823.790798    2525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743380823.790799    2525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743380823.790799    2525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-31 00:27:03.793872: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: /work/tmp/gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: /work/tmp/gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: /work/tmp/gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: /work/tmp/gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: /work/tmp/gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: /work/tmp/gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: /work/tmp/gpt2/124M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from chap4 import GPTModel\n",
    "from chap5 import load_weights_into_gpt\n",
    "\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,         #1\n",
    "    \"context_length\": 1024,      #2\n",
    "    \"drop_rate\": 0.0,            #3\n",
    "    \"qkv_bias\": True             #4\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size, models_dir=f\"{base_path}/gpt2\"\n",
    ")\n",
    "\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50d6cc95-b0a1-4687-909c-b999b5801caa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:11.406541Z",
     "iopub.status.busy": "2025-03-31T00:27:11.406315Z",
     "iopub.status.idle": "2025-03-31T00:27:11.730502Z",
     "shell.execute_reply": "2025-03-31T00:27:11.730236Z",
     "shell.execute_reply.started": "2025-03-31T00:27:11.406530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "from chap4 import generate_text_simple\n",
    "from chap5 import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer).to(device),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8f0e2d4-61d2-47cd-b2a4-a85a6f105a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:11.730931Z",
     "iopub.status.busy": "2025-03-31T00:27:11.730806Z",
     "iopub.status.idle": "2025-03-31T00:27:11.735290Z",
     "shell.execute_reply": "2025-03-31T00:27:11.735070Z",
     "shell.execute_reply.started": "2025-03-31T00:27:11.730921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='tanh')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9697809a-7a7f-4efb-8fd4-8523677c1399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:11.735646Z",
     "iopub.status.busy": "2025-03-31T00:27:11.735545Z",
     "iopub.status.idle": "2025-03-31T00:27:21.636085Z",
     "shell.execute_reply": "2025-03-31T00:27:21.635817Z",
     "shell.execute_reply.started": "2025-03-31T00:27:11.735638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "from chap6 import calc_accuracy_loader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, model, device, num_batches=10\n",
    ")\n",
    "val_accuracy = calc_accuracy_loader(\n",
    "    val_loader, model, device, num_batches=10\n",
    ")\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf88df04-b5a8-4d4c-ae18-dc81397c6e1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:21.636537Z",
     "iopub.status.busy": "2025-03-31T00:27:21.636411Z",
     "iopub.status.idle": "2025-03-31T00:27:21.639316Z",
     "shell.execute_reply": "2025-03-31T00:27:21.639040Z",
     "shell.execute_reply.started": "2025-03-31T00:27:21.636527Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))    #1\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"LoRALayer(in_dim={self.in_dim}, out_dim={self.out_dim}, rank={self.rank}, alpha={self.alpha})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84ae8a8a-a819-404d-895a-333e9449d4c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:21.639711Z",
     "iopub.status.busy": "2025-03-31T00:27:21.639616Z",
     "iopub.status.idle": "2025-03-31T00:27:21.653434Z",
     "shell.execute_reply": "2025-03-31T00:27:21.653047Z",
     "shell.execute_reply.started": "2025-03-31T00:27:21.639702Z"
    }
   },
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1882a532-07b6-43e9-bc48-a10927c391cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:21.653699Z",
     "iopub.status.busy": "2025-03-31T00:27:21.653622Z",
     "iopub.status.idle": "2025-03-31T00:27:21.661005Z",
     "shell.execute_reply": "2025-03-31T00:27:21.660628Z",
     "shell.execute_reply.started": "2025-03-31T00:27:21.653692Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):     #1\n",
    "            print(module)\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:    #2\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18fd3ca8-e6e5-4ac4-989c-0677c8a9e1bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:21.661341Z",
     "iopub.status.busy": "2025-03-31T00:27:21.661256Z",
     "iopub.status.idle": "2025-03-31T00:27:21.669039Z",
     "shell.execute_reply": "2025-03-31T00:27:21.668748Z",
     "shell.execute_reply.started": "2025-03-31T00:27:21.661333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before: 124,441,346\n",
      "Total trainable parameters after: 0\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c66c70fc-bb0d-4069-87b0-cc74d84246d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:21.669435Z",
     "iopub.status.busy": "2025-03-31T00:27:21.669328Z",
     "iopub.status.idle": "2025-03-31T00:27:21.696583Z",
     "shell.execute_reply": "2025-03-31T00:27:21.696309Z",
     "shell.execute_reply.started": "2025-03-31T00:27:21.669423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2, bias=True)\n",
      "Total trainable LoRA parameters: 2,666,528\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a542a652-183b-4aba-b2b9-5d7da0408256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:21.697123Z",
     "iopub.status.busy": "2025-03-31T00:27:21.696917Z",
     "iopub.status.idle": "2025-03-31T00:27:21.705042Z",
     "shell.execute_reply": "2025-03-31T00:27:21.704786Z",
     "shell.execute_reply.started": "2025-03-31T00:27:21.697107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer(in_dim=768, out_dim=3072, rank=16, alpha=16)\n",
      "          )\n",
      "          (1): GELU(approximate='tanh')\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer(in_dim=3072, out_dim=768, rank=16, alpha=16)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer(in_dim=768, out_dim=3072, rank=16, alpha=16)\n",
      "          )\n",
      "          (1): GELU(approximate='tanh')\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer(in_dim=3072, out_dim=768, rank=16, alpha=16)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer(in_dim=768, out_dim=3072, rank=16, alpha=16)\n",
      "          )\n",
      "          (1): GELU(approximate='tanh')\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer(in_dim=3072, out_dim=768, rank=16, alpha=16)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer(in_dim=768, out_dim=3072, rank=16, alpha=16)\n",
      "          )\n",
      "          (1): GELU(approximate='tanh')\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer(in_dim=3072, out_dim=768, rank=16, alpha=16)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer(in_dim=768, out_dim=3072, rank=16, alpha=16)\n",
      "          )\n",
      "          (1): GELU(approximate='tanh')\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer(in_dim=3072, out_dim=768, rank=16, alpha=16)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer(in_dim=768, out_dim=3072, rank=16, alpha=16)\n",
      "          )\n",
      "          (1): GELU(approximate='tanh')\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer(in_dim=3072, out_dim=768, rank=16, alpha=16)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer(in_dim=768, out_dim=3072, rank=16, alpha=16)\n",
      "          )\n",
      "          (1): GELU(approximate='tanh')\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer(in_dim=3072, out_dim=768, rank=16, alpha=16)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer(in_dim=768, out_dim=3072, rank=16, alpha=16)\n",
      "          )\n",
      "          (1): GELU(approximate='tanh')\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer(in_dim=3072, out_dim=768, rank=16, alpha=16)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer(in_dim=768, out_dim=3072, rank=16, alpha=16)\n",
      "          )\n",
      "          (1): GELU(approximate='tanh')\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer(in_dim=3072, out_dim=768, rank=16, alpha=16)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer(in_dim=768, out_dim=3072, rank=16, alpha=16)\n",
      "          )\n",
      "          (1): GELU(approximate='tanh')\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer(in_dim=3072, out_dim=768, rank=16, alpha=16)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer(in_dim=768, out_dim=3072, rank=16, alpha=16)\n",
      "          )\n",
      "          (1): GELU(approximate='tanh')\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer(in_dim=3072, out_dim=768, rank=16, alpha=16)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer(in_dim=768, out_dim=768, rank=16, alpha=16)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer(in_dim=768, out_dim=3072, rank=16, alpha=16)\n",
      "          )\n",
      "          (1): GELU(approximate='tanh')\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer(in_dim=3072, out_dim=768, rank=16, alpha=16)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearWithLoRA(\n",
      "    (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (lora): LoRALayer(in_dim=768, out_dim=2, rank=16, alpha=16)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8919ebde-f911-465b-93a2-7e79e0cd1657",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:21.705609Z",
     "iopub.status.busy": "2025-03-31T00:27:21.705359Z",
     "iopub.status.idle": "2025-03-31T00:27:32.967680Z",
     "shell.execute_reply": "2025-03-31T00:27:32.967328Z",
     "shell.execute_reply.started": "2025-03-31T00:27:21.705593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, model, device, num_batches=10\n",
    ")\n",
    "val_accuracy = calc_accuracy_loader(\n",
    "    val_loader, model, device, num_batches=10\n",
    ")\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b637c915-8b44-4954-996e-63b95e891114",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:27:32.968092Z",
     "iopub.status.busy": "2025-03-31T00:27:32.967996Z",
     "iopub.status.idle": "2025-03-31T00:37:48.152457Z",
     "shell.execute_reply": "2025-03-31T00:37:48.151996Z",
     "shell.execute_reply.started": "2025-03-31T00:27:32.968084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.820, Val loss 3.462\n",
      "Ep 1 (Step 000050): Train loss 0.396, Val loss 0.364\n",
      "Ep 1 (Step 000100): Train loss 0.111, Val loss 0.229\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 2 (Step 000150): Train loss 0.135, Val loss 0.073\n",
      "Ep 2 (Step 000200): Train loss 0.010, Val loss 0.046\n",
      "Ep 2 (Step 000250): Train loss 0.024, Val loss 0.159\n",
      "Training accuracy: 97.50% | Validation accuracy: 97.50%\n",
      "Ep 3 (Step 000300): Train loss 0.019, Val loss 0.122\n",
      "Ep 3 (Step 000350): Train loss 0.036, Val loss 0.264\n",
      "Training accuracy: 100.00% | Validation accuracy: 90.00%\n",
      "Ep 4 (Step 000400): Train loss 0.119, Val loss 0.038\n",
      "Ep 4 (Step 000450): Train loss 0.032, Val loss 0.167\n",
      "Ep 4 (Step 000500): Train loss 0.003, Val loss 0.418\n",
      "Training accuracy: 100.00% | Validation accuracy: 92.50%\n",
      "Ep 5 (Step 000550): Train loss 0.002, Val loss 0.384\n",
      "Ep 5 (Step 000600): Train loss 0.001, Val loss 0.008\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 10.25 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from chap6 import train_classifier_simple\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
    "    train_classifier_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65540252-8654-4fdc-a69d-4068cbd43132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:42:35.999965Z",
     "iopub.status.busy": "2025-03-31T00:42:35.999451Z",
     "iopub.status.idle": "2025-03-31T00:42:36.379074Z",
     "shell.execute_reply": "2025-03-31T00:42:36.378826Z",
     "shell.execute_reply.started": "2025-03-31T00:42:35.999928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from chap6 import plot_values\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(\n",
    "    epochs_tensor, examples_seen_tensor, \n",
    "    train_losses, val_losses, label=\"loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8686a6d0-1902-4476-9015-bc1f191e607b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T00:43:01.832909Z",
     "iopub.status.busy": "2025-03-31T00:43:01.831815Z",
     "iopub.status.idle": "2025-03-31T00:44:11.738441Z",
     "shell.execute_reply": "2025-03-31T00:44:11.737869Z",
     "shell.execute_reply.started": "2025-03-31T00:43:01.832860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 99.81%\n",
      "Validation accuracy: 97.32%\n",
      "Test accuracy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe05cc-72a1-4c66-858d-8220922379e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
